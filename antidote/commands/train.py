"""
Train a new Antidote model.
"""

import argparse
from contextlib import contextmanager
import datetime
import pandas as pd
from pathlib import Path
import random
import requests
import time
import warnings
from yaml import load
import logging

try:
    from yaml import CLoader as Loader
except ImportError:
    from yaml import Loader

from antidote.utils import antidote_builder
from antidote.utils import class3D_balancer
from antidote.utils import class3D_builder
from antidote.utils import tensorflow_tools

warnings.filterwarnings("ignore")

logger = logging.getLogger(__name__)


def add_args(parser):
    """
    Add command line arguments to the parser.

    Args:
    - parser (argparse.ArgumentParser): Argument parser object.

    Returns:
    - argparse.ArgumentParser: Parser with added arguments.
    """

    parser.add_argument(
        "--balance",
        type=int,
        default=-1,
        help="Balance True and False particles in the training set using the specified number of particles for each. By default, jobs with fewer than the requested particle number will be correctly balanced to the smaller value.",
    )

    parser.add_argument(
        "--batchsize",
        type=int,
        default=256,
        help="The batch size to use for model training. Note that if training is run from the tuner, the batch size is chosen automatically.",
    )

    parser.add_argument(
        "--chunksize",
        type=int,
        help="The size of the data to load for hyperparameter tuning and training. This determines the size of data that is loaded into RAM for passing to the tuner and training functionâ€“the batchsize for training can be set explicitly with the --batchsize argument.",
    )

    parser.add_argument(
        "--from-hdf5",
        type=Path,
        help="Path to HDF5 file with labeled training data.",
    )

    parser.add_argument(
        "--from-model",
        type=Path,
        help="Path to a Keras model file or TF checkpoint directory containing initial model configuration and weights",
    )

    parser.add_argument(
        "--from-yaml",
        type=Path,
        help="Path to a YAML file containing a spec for the training data.",
    )

    parser.add_argument(
        "--iteration-count",
        type=int,
        default=25,
        help="Set iteration count for training data.",
    )

    parser.add_argument(
        "--labeling-function",
        type=str,
        default="true",
        help="A string or lambda function to use for assigning labels in training data.",
    )

    parser.add_argument(
        "--max-workers",
        type=int,
        default=None,
        help="Number of cores to use for starfile parsing.",
    )

    parser.add_argument(
        "--min-iteration",
        type=int,
        default=2,
        help="The minimum iteration to consider during inference. The model must also accept this number of iterations. The recommended value is 2.",
    )

    parser.add_argument(
        "--only-build-training-set",
        action="store_true",
        help="Skip training (only build the training file).",
    )

    parser.add_argument(
        "--objective",
        type=str,
        default="auc",
        choices=["auc", "acc"],
        help="Objective metric to optimize during hyperparameter search. Use 'auc' for validation auc and 'acc' for validation accuracy.",
    )

    # This doesn't work as expected due to the order of device initialization by TensorFlow
    # parser.add_argument(
    #     "--only-use-cpu",
    #     action="store_true",
    #     help="Ignore GPUs and only use CPUs.",
    # )

    parser.add_argument(
        "--output-path",
        "--output",
        "-o",
        dest="output_path",
        type=Path,
        default=Path.cwd(),
        help="Path to model output.",
    )

    parser.add_argument(
        "--overwrite",
        action="store_true",
        default=False,
        help="Flag for overwriting the output file.",
    )

    parser.add_argument(
        "--run-exhaustive-tuner",
        action="store_true",
        help="Perform a large Keras Tuner search. This will save the best 100 models as full model files.",
    )

    parser.add_argument(
        "--slurm-id",
        type=int,
        help="Append the slurm job ID (or any other value) to directories generated by antidote.",
    )

    parser.add_argument(
        "--testing-path",
        dest="testing_path",
        type=Path,
        action="append",
        default=None,
        help="Path to a testing dataset.",
    )

    parser.add_argument(
        "--use-mha",
        action="store_true",
        help="Add multi-head attention to the model architecture.",
    )

    return parser


@contextmanager
def hdf5_manager(path, mode="a"):
    """
    Context manager for handling HDF5 file writing.
    """
    store = pd.HDFStore(path, mode=mode)
    try:
        yield store
    finally:
        store.close()


def train_from_model(
    model_path,
    hdf5_path,
    chunksize,
    batch_size,
    output_path="antidote",
    slurm_id=None,
    testing_path=None,
    words=True,
):
    """
    Skip hyperparameter tuning an train a model directly from a model and a training data set.
    """

    if slurm_id:
        output_path = f"{output_path}_{slurm_id}"

    output_path = f"{output_path}_{str((int(time.time())))}"

    if words:
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            response = requests.get(
                "https://rayberkeley.com/posts/words.txt",
                headers=headers,
            )
            if response.status_code == 200:
                word = random.choice(response.text.splitlines())
                output_path = f"{output_path}_{word}"
            else:
                logger.error(f"Random word list access failed with HTTP response {response}")
        except Exception as e:
            logger.error(f"Word assignment failed: {e}")

    model_path = Path(model_path)
    hdf5_path = Path(hdf5_path)

    pre_model = tensorflow_tools.load_model(model_path)

    model = antidote_builder.train_model(
        pre_model,
        hdf5_path,
        batch_size=batch_size,
        chunksize=chunksize,
        output_path=output_path,
        testing_path=testing_path,
    )

    return model


def train_from_hdf5(
    hdf5_path,
    chunksize=None,
    key: str = "data",
    use_mha=False,
    objective="auc",
    output_path="antidote",
    run_exhaustive_tuner=None,
    slurm_id=None,
    testing_path=None,
    words=True,
):
    """
    Entry into the training functionality.
    """
    if slurm_id:
        output_path = f"{output_path}_{slurm_id}"

    output_path = f"{output_path}_{str((int(time.time())))}"

    if words:
        try:
            headers = {"User-Agent": "Mozilla/5.0"}
            response = requests.get(
                "https://rayberkeley.com/posts/words.txt",
                headers=headers,
            )
            if response.status_code == 200:
                word = random.choice(response.text.splitlines())
                output_path = f"{output_path}_{word}"
            else:
                logger.error(f"Random word list access failed with HTTP response {response}")
        except Exception as e:
            logger.error(f"Word assignment failed: {e}")

    if run_exhaustive_tuner:
        pre_model, pre_model_params = antidote_builder.perform_hyperparameter_search(
            hdf5_path,
            chunksize=chunksize,
            factor=2,
            hdf5_key=key,
            hyperband_iterations=3,
            max_epochs=150,
            num_models=20,
            objective=objective,
            output_path=output_path,
            use_mha=use_mha,
        )

        # Don't continue with training, just generate a large number of models.
        return None

    else:
        pre_model, pre_model_params = antidote_builder.perform_hyperparameter_search(
            hdf5_path,
            chunksize,
            hdf5_key=key,
            objective=objective,
            output_path=output_path,
            use_mha=use_mha,
        )

        logger.info(f"Hyperparameter search complete. Training model with best parameters:")
        logger.info(pre_model.summary())

        best_batch_size = pre_model_params.hyperparameters.get("batch_size")

        model = antidote_builder.train_model(
            pre_model,
            hdf5_path,
            batch_size=best_batch_size,
            chunksize=chunksize,
            output_path=output_path,
            testing_path=testing_path,
        )

        return model


def train_from_yaml(
    input_path: Path,
    balance: int = -1,
    iteration_count: int = 25,
    output_path: Path = "./antidote_training_data",
    max_workers: int = None,
    only_build_training_set: bool = False,
) -> Path:
    """
    Parse a YAML config file and use it to construct an HDF5 file that will be used for training.
    """
    # handle output path structure
    input_path = Path(input_path)
    output_path = Path(output_path)
    output_path.mkdir(parents=True, exist_ok=True)
    hdf5_path = output_path / f"{output_path.name}_{int(time.time())}.h5"

    with hdf5_manager(hdf5_path, mode="a") as hdf5_store:
        with open(input_path, "r") as stream:
            data = load(stream, Loader=Loader)
            jobs = data["jobs"]
            particle_index = 0  # to reindex the training df

            for job, args in jobs.items():
                logger.info(f"Parsing {job}...")
                # Handle ambiguity in labeling_func expected by as_training, and implicit typing performed by YAML
                if "labeling_func" in args:
                    warnings.warn(
                        "The labeling_func is set directly in the config file. When using a YAML file "
                        "to build the training dataset, it is recommended to set the type of the labeling "
                        "approach explicitly with either the `lambda` or `label` attributes. See the "
                        "example config file for more information."
                    )
                    if args["labeling_func"].startswith("lambda"):
                        logger.info("labeling_func inferred as lambda function.")
                        labeling_func = eval(args.pop("labeling_func"))
                    else:
                        logger.info("labeling_func inferred as string.")
                        labeling_func = str(args.pop("labeling_func"))
                elif "lambda" in args:
                    labeling_func = eval(args.pop("lambda"))
                elif "label" in args:
                    labeling_func = str(args.pop("label"))
                else:
                    logger.fatal("Could not find a valid labeling approach, check YAML configuration.")

                try:
                    parsed_job = class3D_builder.RelionClass3DJob.as_training(
                        job,
                        labeling_func=labeling_func,
                        max_workers=max_workers,
                        **args,
                    )

                    if parsed_job.data is None:
                        logger.warning(f"{parsed_job.name} failed to parse.")
                    else:
                        if parsed_job.num_iterations != iteration_count:
                            logger.warning(
                                f"{parsed_job.name} has {parsed_job.num_iterations} iterations, training data expects {iteration_count}."
                            )
                        else:
                            # Manage numerical index to facilitate random access during training
                            parsed_job.data.reset_index(drop=True, inplace=True)
                            parsed_job.data.index = range(
                                particle_index,
                                particle_index + len(parsed_job.data),
                            )
                            particle_index += len(parsed_job.data)

                            # Update dtypes for HDF5 compatibility
                            parsed_job.data["Label"] = parsed_job.data["Label"].astype(bool)

                            if balance > 0:
                                logger.info(f"Balancing data to {balance} samples per label...")
                                class3D_balancer.balance(parsed_job, balance)

                            # Append the modified DataFrame to the HDF5 file
                            hdf5_store.append(
                                "data",
                                parsed_job.data,
                                format="table",
                            )

                    del parsed_job

                except Exception as e:
                    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    logger.error(f"Error parsing job at {current_time}: {e}")
                    warnings.warn(f"Error parsing job at {current_time}: {e}")

    return hdf5_path


def main(args):
    # Configure GPUs if they're present
    tensorflow_tools.configure_gpus()

    if args.from_model and args.from_hdf5:
        logger.info(f"Loading model from {args.from_model}...")
        logger.info(f"Loading training data from {args.from_hdf5}...")
        if args.testing_path:
            logger.info(f"Loading testing data from {', '.join(map(str, args.testing_path))}...")

        train_from_model(
            args.from_model,
            args.from_hdf5,
            batch_size=args.batchsize,
            chunksize=args.chunksize,
            testing_path=args.testing_path,
            slurm_id=args.slurm_id if args.slurm_id else None,
        )
        return

    hdf5_path = args.from_hdf5

    # if training from yaml, first build the training set
    if args.from_yaml:
        logger.info(f"Loading training data from {args.from_yaml}...")
        hdf5_path = train_from_yaml(
            args.from_yaml,
            args.balance,
            args.iteration_count,
            args.output_path,
            args.max_workers,
        )

    # whether training from existing hdf5 file or a just-generated-via-yaml hdf5 file, train the model
    if args.from_hdf5 or (args.from_yaml and not args.only_build_training_set):
        logger.info(f"Loading training data from {args.from_hdf5}...")
        if args.testing_path:
            logger.info(f"Loading testing data from {', '.join(map(str, args.testing_path))}...")

        train_from_hdf5(
            hdf5_path,
            chunksize=args.chunksize,
            objective=args.objective,
            testing_path=args.testing_path,
            run_exhaustive_tuner=args.run_exhaustive_tuner,
            slurm_id=args.slurm_id if args.slurm_id else None,
            use_mha=args.use_mha,
        )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description=__doc__)
    args = add_args(parser).parse_args()

    if args.only_build_training_set and args.from_yaml is None:
        parser.error("The --only-build-training-set argument is not valid for pre-built training sets.")

    if args.from_model and args.from_hdf5 is None:
        parser.error("The --from-model argument must be passed alongside an HDF5 file using the --from-hdf5 argument.")

    if args.run_exhaustive_tuner and args.from_hdf5 is None:
        parser.error(
            "The --run-exhaustive-tuner argument must be passed alongside an HDF5 file using the --from-hdf5 argument."
        )

    main(args)
